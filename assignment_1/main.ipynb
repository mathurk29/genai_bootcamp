{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathurk29/git/EPAM/llm_application_bootcamp_2/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lang    model  tokens\n",
      "0   english   gpt-4o      46\n",
      "1   english    gpt-4      46\n",
      "2   english  gpt-3.5      46\n",
      "3   english  mistral      50\n",
      "4   english    llama      51\n",
      "5   spanish   gpt-4o      54\n",
      "6   spanish    gpt-4      70\n",
      "7   spanish  gpt-3.5      70\n",
      "8   spanish  mistral      76\n",
      "9   spanish    llama      73\n",
      "10   arabic   gpt-4o      67\n",
      "11   arabic    gpt-4     113\n",
      "12   arabic  gpt-3.5     113\n",
      "13   arabic  mistral     151\n",
      "14   arabic    llama     152\n",
      "15    hindi   gpt-4o      59\n",
      "16    hindi    gpt-4     185\n",
      "17    hindi  gpt-3.5     185\n",
      "18    hindi  mistral     186\n",
      "19    hindi    llama     190\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from adapter import (GPTTokenizerAdapter, LlamaTokenizerAdapter,\n",
    "                     MistralTokenizerAdapter)\n",
    "from sample_texts import sample_texts\n",
    "\n",
    "gpt_models = [\"gpt-4o\", \"gpt-4\", \"gpt-3.5\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = []\n",
    "    for lang, text in sample_texts.items():\n",
    "        for model in gpt_models:\n",
    "            gpt_adapter = GPTTokenizerAdapter(model)\n",
    "            gpt_tokens = gpt_adapter.count_tokens(text)\n",
    "            result.append({\"lang\": lang, \"model\": model, \"tokens\": gpt_tokens})\n",
    "\n",
    "        mistral_adapter = MistralTokenizerAdapter()\n",
    "        mistral_tokens = mistral_adapter.count_tokens(text)\n",
    "        result.append({\"lang\": lang, \"model\": \"mistral\", \"tokens\": mistral_tokens})\n",
    "\n",
    "        llama_adapter = LlamaTokenizerAdapter()\n",
    "        llama_tokens = llama_adapter.count_tokens(text)\n",
    "        result.append({\"lang\": lang, \"model\": \"llama\", \"tokens\": llama_tokens})\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenization</h1>\n",
    "\n",
    "- Language models, or any machine learning models, process numbers not text.\n",
    "- So, to use language models, we need to convert our text into numbers.\n",
    "- Tokenization breaks down text into tokens, and each token is assigned a numerical representation, or index, which can be used to feed into a model\n",
    "- Each unique token is assigned a specific index number in the tokenizer’s vocabulary.\n",
    "- Tokenization is done during pre-training of LLM.\n",
    "- these tokens are passed through the model, which typically includes an embedding layer and transformer blocks\n",
    "- The embedding layer converts the tokens into dense vectors that capture semantic meanings\n",
    "- The transformer blocks then process these embedding vectors to understand the context\n",
    "- The last step is decoding, which detokenize output tokens back to human-readable text. This is done by mapping the tokens back to their corresponding words using the tokenizer’s vocabulary.\n",
    "\n",
    "<h1>Tokenization Techniques</h1>\n",
    "\n",
    "1. **Word-Based Tokenization** : The most straightforward form, where the text is segmented into words based on spaces or punctuation. It's simple but  inefficient for languages without clear word delimiters.\n",
    "Also the vocabulary size is going to be unreasonably huge as each word has to included.\n",
    "\n",
    "2. **Subword Tokenization**: Popularized by models like **BERT** and **GPT**, this approach involves breaking down words into smaller, meaningful units (subwords) using algorithms like Byte-Pair Encoding (**BPE**) or **WordPiece** by Google. This method helps in managing vocabulary size more efficiently and dealing with unknown words or morphological variations.\n",
    "\n",
    "**Wordpiece Algo**:\n",
    "Make vocabulary from each letter.\n",
    "For every pair of letters:\n",
    "score = frequency of pair / frequency of first letter * frequency of second letter\n",
    "The letter pair with top score is added to the vocabulary.\n",
    "Now the split corpus is updated based on the the new vocabulary.\n",
    "This is repeated till desired level is reached.\n",
    "\n",
    "Wordpiece is developed by Google.\n",
    "\n",
    "**BPE Ex**:\n",
    "- <ins>aa</ins>abd<ins>aa</ins>abac   \n",
    "- Z<ins>ab</ins>dZ<ins>ab</ins>       *Here aa is tokenized to z *\n",
    "- <ins>ZX</ins>d<ins>ZX</ins>ac ac    *Here ab is tokenized to X *\n",
    "- sdsac       *Here ZX is tokenized to S*\n",
    "\n",
    "Tiktoken is Python library for BPE. (gpt2, cl100k_base, O200k,base, r50k,base, etc)\n",
    "\n",
    "The word is base64 encoded and numerical value assigned to that encoding is the encoded value of the word.\n",
    "\n",
    "Welcome -> Base64 encoded -> V2VS429t -> in cl100k_base _> 14262\n",
    "\n",
    "BPE is used by OpenAI.\n",
    "\n",
    "3. **Byte-Level Tokenization**: As seen in models like GPT-2 and GPT-3, this approach tokenizes text at the byte level, encoding each byte of the text into tokens, which aligns neatly with UTF-8 encoding, ensuring better handling of diverse languages and special characters.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
