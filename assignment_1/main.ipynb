{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathurk29/git/EPAM/llm_application_bootcamp_2/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lang    model  tokens\n",
      "0   english   gpt-4o      46\n",
      "1   english    gpt-4      46\n",
      "2   english  gpt-3.5      46\n",
      "3   english  mistral      50\n",
      "4   english    llama      51\n",
      "5   spanish   gpt-4o      54\n",
      "6   spanish    gpt-4      70\n",
      "7   spanish  gpt-3.5      70\n",
      "8   spanish  mistral      76\n",
      "9   spanish    llama      73\n",
      "10   arabic   gpt-4o      67\n",
      "11   arabic    gpt-4     113\n",
      "12   arabic  gpt-3.5     113\n",
      "13   arabic  mistral     151\n",
      "14   arabic    llama     152\n",
      "15    hindi   gpt-4o      59\n",
      "16    hindi    gpt-4     185\n",
      "17    hindi  gpt-3.5     185\n",
      "18    hindi  mistral     186\n",
      "19    hindi    llama     190\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from adapter import (GPTTokenizerAdapter, LlamaTokenizerAdapter,\n",
    "                     MistralTokenizerAdapter)\n",
    "from sample_texts import sample_texts\n",
    "\n",
    "gpt_models = [\"gpt-4o\", \"gpt-4\", \"gpt-3.5\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = []\n",
    "    for lang, text in sample_texts.items():\n",
    "        for model in gpt_models:\n",
    "            gpt_adapter = GPTTokenizerAdapter(model)\n",
    "            gpt_tokens = gpt_adapter.count_tokens(text)\n",
    "            result.append({\"lang\": lang, \"model\": model, \"tokens\": gpt_tokens})\n",
    "\n",
    "        mistral_adapter = MistralTokenizerAdapter()\n",
    "        mistral_tokens = mistral_adapter.count_tokens(text)\n",
    "        result.append({\"lang\": lang, \"model\": \"mistral\", \"tokens\": mistral_tokens})\n",
    "\n",
    "        llama_adapter = LlamaTokenizerAdapter()\n",
    "        llama_tokens = llama_adapter.count_tokens(text)\n",
    "        result.append({\"lang\": lang, \"model\": \"llama\", \"tokens\": llama_tokens})\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenization</h1>\n",
    "\n",
    "- Texts need to be represented as numbers in our models so that our model can understand them correctly. \n",
    "- Tokenization breaks down text into tokens, and each token is assigned a numerical representation, or index, which can be used to feed into a model\n",
    "- Each unique token is assigned a specific index number in the tokenizer’s vocabulary.\n",
    "- these tokens are passed through the model, which typically includes an embedding layer and transformer blocks\n",
    "- The embedding layer converts the tokens into dense vectors that capture semantic meanings\n",
    "- The transformer blocks then process these embedding vectors to understand the context\n",
    "- The last step is decoding, which detokenize output tokens back to human-readable text. This is done by mapping the tokens back to their corresponding words using the tokenizer’s vocabulary.\n",
    "\n",
    "<h2>Types of Tokenization</h2>\n",
    "\n",
    "1. **Word-Based Tokenization** : This is the most straightforward form, where the text is segmented into words based on spaces or punctuation. It's simple but can be inefficient for languages without clear word delimiters or for handling inflections and derivations effectively.\n",
    "\n",
    "2. **Subword Tokenization**: Popularized by models like BERT and GPT, this approach involves breaking down words into smaller, meaningful units (subwords) using algorithms like Byte-Pair Encoding (BPE) or WordPiece. This method helps in managing vocabulary size more efficiently and dealing with unknown words or morphological variations.\n",
    "\n",
    "3. **Byte-Level Tokenization**: As seen in models like GPT-2 and GPT-3, this approach tokenizes text at the byte level, encoding each byte of the text into tokens, which aligns neatly with UTF-8 encoding, ensuring better handling of diverse languages and special characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
